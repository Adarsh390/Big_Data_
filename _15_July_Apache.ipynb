{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Working with RDDs:\n",
    "#    a) Write a Python program to create an RDD from a local data source.\n",
    "#    b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
    "#    c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate.\n",
    "#Sol:\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext(\"local\", \"RDD Example\")\n",
    "\n",
    "# Create an RDD from a local data source\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Implement transformations and actions on the RDD\n",
    "# Map operation: multiply each element by 2\n",
    "mapped_rdd = rdd.map(lambda x: x * 2)\n",
    "\n",
    "# Filter operation: filter even numbers\n",
    "filtered_rdd = mapped_rdd.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "# Reduce operation: sum all the elements\n",
    "sum_result = filtered_rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Print the result\n",
    "print(\"Sum of even numbers multiplied by 2:\", sum_result)\n",
    "\n",
    "# Aggregate operation: calculate sum and count of elements\n",
    "sum_count = filtered_rdd.aggregate((0, 0),\n",
    "                                  lambda acc, value: (acc[0] + value, acc[1] + 1),\n",
    "                                  lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))\n",
    "\n",
    "print(\"Sum:\", sum_count[0])\n",
    "print(\"Count:\", sum_count[1])\n",
    "\n",
    "# Stop SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Spark DataFrame Operations:\n",
    "#    a) Write a Python program to load a CSV file into a Spark DataFrame.\n",
    "#    b)Perform common DataFrame operations such as filtering, grouping, or joining.\n",
    "#    c) Apply Spark SQL queries on the DataFrame to extract insights from the data.\n",
    "#Sol:\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrame Example\").getOrCreate()\n",
    "\n",
    "# Load CSV file into DataFrame\n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display the DataFrame schema\n",
    "df.printSchema()\n",
    "\n",
    "# Perform filtering operation\n",
    "filtered_df = df.filter(df[\"age\"] > 30)\n",
    "\n",
    "# Perform grouping operation\n",
    "grouped_df = df.groupBy(\"gender\").count()\n",
    "\n",
    "# Perform joining operation\n",
    "joined_df = df.join(grouped_df, on=\"gender\")\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "filtered_df.show()\n",
    "\n",
    "# Display the grouped DataFrame\n",
    "grouped_df.show()\n",
    "\n",
    "# Display the joined DataFrame\n",
    "joined_df.show()\n",
    "\n",
    "# Apply Spark SQL queries on the DataFrame\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Query to get the average age\n",
    "average_age = spark.sql(\"SELECT AVG(age) FROM people\")\n",
    "\n",
    "# Query to get the count of people by gender\n",
    "gender_count = spark.sql(\"SELECT gender, COUNT(*) FROM people GROUP BY gender\")\n",
    "\n",
    "# Display the results\n",
    "average_age.show()\n",
    "gender_count.show()\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Spark Streaming:\n",
    "#   a) Write a Python program to create a Spark Streaming application.\n",
    "#    b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket).\n",
    "#    c) Implement streaming transformations and actions to process and analyze the incoming data stream.\n",
    "#Sol\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Create a StreamingContext with a batch interval of 1 second\n",
    "ssc = StreamingContext(sparkContext, 1)\n",
    "\n",
    "# Configure Kafka parameters\n",
    "kafka_params = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    \"auto.offset.reset\": \"smallest\",\n",
    "    \"group.id\": \"spark-streaming\"\n",
    "}\n",
    "\n",
    "# Create a DStream that represents the data stream from Kafka topic(s)\n",
    "kafka_topic = \"my-topic\"\n",
    "dstream = KafkaUtils.createDirectStream(ssc, [kafka_topic], kafka_params)\n",
    "\n",
    "# Perform transformations and actions on the DStream\n",
    "# Example: Count the occurrences of each word in the stream\n",
    "word_counts = dstream \\\n",
    "    .flatMap(lambda x: x[1].split(\" \")) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Print the word counts\n",
    "word_counts.pprint()\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the streaming to finish\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Spark SQL and Data Source Integration:\n",
    "#    a) Write a Python program to connect Spark with a relational database (e.g., MySQL, PostgreSQL).\n",
    "#    b)Perform SQL operations on the data stored in the database using Spark SQL.\n",
    "#    c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3.\n",
    "#sol\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL Example\").getOrCreate()\n",
    "\n",
    "# Connect Spark with a MySQL database\n",
    "jdbc_url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
    "connection_properties = {\n",
    "    \"user\": \"username\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# Load data from a MySQL table into a DataFrame using Spark SQL\n",
    "df = spark.read.jdbc(jdbc_url, \"tablename\", properties=connection_properties)\n",
    "\n",
    "# Perform SQL operations on the DataFrame\n",
    "df.createOrReplaceTempView(\"mytable\")\n",
    "\n",
    "# Execute SQL queries using Spark SQL\n",
    "result = spark.sql(\"SELECT * FROM mytable WHERE column1 > 100\")\n",
    "\n",
    "# Display the result\n",
    "result.show()\n",
    "\n",
    "# Read data from HDFS or S3 into a DataFrame\n",
    "hdfs_path = \"hdfs://localhost:9000/path/to/file.csv\"\n",
    "s3_path = \"s3a://bucket-name/path/to/file.csv\"\n",
    "\n",
    "hdfs_df = spark.read.csv(hdfs_path, header=True, inferSchema=True)\n",
    "s3_df = spark.read.csv(s3_path, header=True, inferSchema=True)\n",
    "\n",
    "# Perform operations on the DataFrames\n",
    "# Example: Display the schema of the HDFS DataFrame\n",
    "hdfs_df.printSchema()\n",
    "\n",
    "# Example: Display the first 10 rows of the S3 DataFrame\n",
    "s3_df.show(10)\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
