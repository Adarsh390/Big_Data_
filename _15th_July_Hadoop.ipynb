{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n",
    "#Sol\n",
    "import configparser\n",
    "\n",
    "def display_core_components(config_file_path):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file_path)\n",
    "\n",
    "    if 'core-site' in config:\n",
    "        core_components = config['core-site'].get('fs.defaultFS', '').split(',')\n",
    "        print(\"Core Components:\")\n",
    "        for component in core_components:\n",
    "            print(component.strip())\n",
    "\n",
    "# Example usage\n",
    "config_file = '/path/to/hadoop/core-site.xml'\n",
    "display_core_components(config_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.\n",
    "#sol:\n",
    "import pyarrow.hdfs\n",
    "\n",
    "def calculate_directory_size(hdfs_host, hdfs_port, directory_path):\n",
    "    hdfs_client = pyarrow.hdfs.connect(host=hdfs_host, port=hdfs_port)\n",
    "\n",
    "    total_size = 0\n",
    "\n",
    "    # Walk through the directory recursively and calculate the size of each file\n",
    "    for file_info in hdfs_client.walk(directory_path):\n",
    "        if file_info['kind'] == 'file':\n",
    "            total_size += file_info['size']\n",
    "\n",
    "    return total_size\n",
    "\n",
    "# Example usage\n",
    "hdfs_host = 'localhost'\n",
    "hdfs_port = 9000\n",
    "directory_path = '/path/to/hdfs/directory'\n",
    "\n",
    "size = calculate_directory_size(hdfs_host, hdfs_port, directory_path)\n",
    "print(f\"Total file size: {size} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.\n",
    "#Sol:\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import heapq\n",
    "\n",
    "class TopNWords(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(TopNWords, self).configure_args()\n",
    "        self.add_passthru_arg('--topN', type=int, help='Number of top N words to display')\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_topN_words)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        for word in line.strip().split():\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    def reducer_find_topN_words(self, _, counts_words):\n",
    "        N = self.options.topN\n",
    "        topN_words = heapq.nlargest(N, counts_words)\n",
    "        for count, word in topN_words:\n",
    "            yield count, word\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TopNWords.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n",
    "#Sol:\n",
    "import requests\n",
    "\n",
    "# Define the base URL for the Hadoop REST API\n",
    "hadoop_url = 'http://<hadoop_host>:<hadoop_port>'\n",
    "\n",
    "def check_namenode_status():\n",
    "    # Endpoint to check NameNode status\n",
    "    namenode_url = f\"{hadoop_url}/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus\"\n",
    "    response = requests.get(namenode_url)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the health status of the NameNode\n",
    "    if 'beans' in data and len(data['beans']) > 0:\n",
    "        status = data['beans'][0]['State']\n",
    "        print(f\"NameNode Health Status: {status}\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve NameNode status.\")\n",
    "\n",
    "def check_datanode_status():\n",
    "    # Endpoint to check DataNode status\n",
    "    datanode_url = f\"{hadoop_url}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
    "    response = requests.get(datanode_url)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the health status of each DataNode\n",
    "    if 'beans' in data and len(data['beans']) > 0:\n",
    "        datanodes = data['beans'][0]['LiveNodes']\n",
    "        print(\"DataNode Health Status:\")\n",
    "        for node in datanodes:\n",
    "            status = datanodes[node]['AdminState']\n",
    "            print(f\"{node}: {status}\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve DataNode status.\")\n",
    "\n",
    "# Example usage\n",
    "check_namenode_status()\n",
    "print()\n",
    "check_datanode_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Develop a Python program that lists all the files and directories in a specific HDFS path.\n",
    "#Sol:\n",
    "import pyarrow.hdfs\n",
    "\n",
    "def list_hdfs_path(hdfs_host, hdfs_port, hdfs_path):\n",
    "    hdfs_client = pyarrow.hdfs.connect(host=hdfs_host, port=hdfs_port)\n",
    "\n",
    "    # List all files and directories in the specified HDFS path\n",
    "    file_info = hdfs_client.ls(hdfs_path, detail=True)\n",
    "\n",
    "    # Print the file and directory names\n",
    "    for item in file_info:\n",
    "        name = item['name']\n",
    "        is_directory = item['kind'] == 'directory'\n",
    "        print(f\"{name} (Directory: {is_directory})\")\n",
    "\n",
    "# Example usage\n",
    "hdfs_host = 'localhost'\n",
    "hdfs_port = 9000\n",
    "hdfs_path = '/path/to/hdfs/directory'\n",
    "\n",
    "list_hdfs_path(hdfs_host, hdfs_port, hdfs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6: Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.\n",
    "#sol:\n",
    "import requests\n",
    "\n",
    "# Define the base URL for the Hadoop REST API\n",
    "hadoop_url = 'http://<hadoop_host>:<hadoop_port>'\n",
    "\n",
    "def analyze_storage_utilization():\n",
    "    # Endpoint to get DataNode information\n",
    "    datanodes_url = f\"{hadoop_url}/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState\"\n",
    "\n",
    "    # Send a GET request to retrieve DataNode information\n",
    "    response = requests.get(datanodes_url)\n",
    "    data = response.json()\n",
    "\n",
    "    if 'beans' in data and len(data['beans']) > 0:\n",
    "        datanodes = data['beans'][0]['StorageInfo']\n",
    "        storage_utilization = {}\n",
    "        for node in datanodes:\n",
    "            node_name = node['datanodeHostName']\n",
    "            capacity = node['capacity']\n",
    "            used = node['used']\n",
    "            utilization = (used / capacity) * 100\n",
    "            storage_utilization[node_name] = utilization\n",
    "\n",
    "        # Identify the node with the highest storage capacity\n",
    "        max_capacity_node = max(storage_utilization, key=storage_utilization.get)\n",
    "        max_capacity = storage_utilization[max_capacity_node]\n",
    "\n",
    "        # Identify the node with the lowest storage capacity\n",
    "        min_capacity_node = min(storage_utilization, key=storage_utilization.get)\n",
    "        min_capacity = storage_utilization[min_capacity_node]\n",
    "\n",
    "        print(\"Storage Utilization:\")\n",
    "        for node, utilization in storage_utilization.items():\n",
    "            print(f\"{node}: {utilization:.2f}%\")\n",
    "\n",
    "        print(f\"\\nNode with highest storage capacity: {max_capacity_node} ({max_capacity:.2f}%)\")\n",
    "        print(f\"Node with lowest storage capacity: {min_capacity_node} ({min_capacity:.2f}%)\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve DataNode information.\")\n",
    "\n",
    "# Example usage\n",
    "analyze_storage_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n",
    "#Sol:\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Define the base URL for the YARN ResourceManager API\n",
    "yarn_url = 'http://<yarn_host>:<yarn_port>'\n",
    "\n",
    "def submit_hadoop_job(jar_path, input_path, output_path):\n",
    "    # Endpoint to submit a Hadoop job\n",
    "    submit_url = f\"{yarn_url}/ws/v1/cluster/apps/new-application\"\n",
    "\n",
    "    # Send a POST request to create a new YARN application\n",
    "    response = requests.post(submit_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Extract the application ID from the response\n",
    "        application_id = response.json()['application-id']\n",
    "\n",
    "        # Submit the Hadoop job using the obtained application ID\n",
    "        submit_job_url = f\"{yarn_url}/ws/v1/cluster/apps/{application_id}/app\"\n",
    "        payload = {\n",
    "            \"application-id\": application_id,\n",
    "            \"application-name\": \"HadoopJob\",\n",
    "            \"am-container-spec\": {\n",
    "                \"commands\": {\n",
    "                    \"command\": f\"hadoop jar {jar_path} <input_path> <output_path>\",\n",
    "                    \"arguments\": [input_path, output_path]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        response = requests.post(submit_job_url, data=payload, headers=headers)\n",
    "\n",
    "        if response.status_code == 202:\n",
    "            print(\"Hadoop job submitted successfully.\")\n",
    "            return application_id\n",
    "\n",
    "    print(\"Failed to submit Hadoop job.\")\n",
    "    return None\n",
    "\n",
    "def monitor_hadoop_job(application_id):\n",
    "    # Endpoint to get the status of a YARN application\n",
    "    status_url = f\"{yarn_url}/ws/v1/cluster/apps/{application_id}\"\n",
    "\n",
    "    # Monitor the Hadoop job progress until it is completed\n",
    "    while True:\n",
    "        response = requests.get(status_url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            app_status = data['app']['finalStatus']\n",
    "            if app_status in ['SUCCEEDED', 'FAILED', 'KILLED']:\n",
    "                break\n",
    "            print(f\"Hadoop job is still running. Progress: {data['app']['progress']}\")\n",
    "        time.sleep(5)\n",
    "\n",
    "    if app_status == 'SUCCEEDED':\n",
    "        print(\"Hadoop job completed successfully.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Hadoop job failed or was killed.\")\n",
    "        return False\n",
    "\n",
    "def retrieve_hadoop_job_output(application_id, output_path):\n",
    "    # Endpoint to retrieve the output of a completed YARN application\n",
    "    output_url = f\"{yarn_url}/ws/v1/cluster/apps/{application_id}/appattempts\"\n",
    "\n",
    "    # Send a GET request to obtain the output of the Hadoop job\n",
    "    response = requests.get(output_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Extract the output location from the response\n",
    "        attempts = response.json()['appAttempts']['appAttempt']\n",
    "        output_location = attempts[-1]['logsLink'] + output_path\n",
    "\n",
    "        # Retrieve the output file contents\n",
    "        response = requests.get(output_location)\n",
    "        if response.status_code == 200:\n",
    "            output = response.text\n",
    "            print(f\"Hadoop job output:\\n{output}\")\n",
    "            return output\n",
    "\n",
    "    print(\"Failed to retrieve Hadoop job output.\")\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "jar_path = '/path/to/hadoop/job.jar'\n",
    "input_path = '/input/path'\n",
    "output_path = '/output/path'\n",
    "\n",
    "application_id = submit_hadoop_job(jar_path, input_path, output_path)\n",
    "if application_id:\n",
    "    if monitor_hadoop_job(application_id):\n",
    "        retrieve_hadoop_job_output(application_id, output_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.\n",
    "#Sol:\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Define the base URL for the YARN ResourceManager API\n",
    "yarn_url = 'http://<yarn_host>:<yarn_port>'\n",
    "\n",
    "def submit_hadoop_job(jar_path, input_path, output_path, vcores, memory):\n",
    "    # Endpoint to submit a Hadoop job\n",
    "    submit_url = f\"{yarn_url}/ws/v1/cluster/apps/new-application\"\n",
    "\n",
    "    # Send a POST request to create a new YARN application\n",
    "    response = requests.post(submit_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Extract the application ID from the response\n",
    "        application_id = response.json()['application-id']\n",
    "\n",
    "        # Submit the Hadoop job using the obtained application ID and set resource requirements\n",
    "        submit_job_url = f\"{yarn_url}/ws/v1/cluster/apps/{application_id}/app\"\n",
    "        payload = {\n",
    "            \"application-id\": application_id,\n",
    "            \"application-name\": \"HadoopJob\",\n",
    "            \"am-container-spec\": {\n",
    "                \"commands\": {\n",
    "                    \"command\": f\"hadoop jar {jar_path} <input_path> <output_path>\",\n",
    "                    \"arguments\": [input_path, output_path]\n",
    "                },\n",
    "                \"resource\": {\n",
    "                    \"vcores\": vcores,\n",
    "                    \"memory\": memory\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        response = requests.post(submit_job_url, json=payload, headers=headers)\n",
    "\n",
    "        if response.status_code == 202:\n",
    "            print(\"Hadoop job submitted successfully.\")\n",
    "            return application_id\n",
    "\n",
    "    print(\"Failed to submit Hadoop job.\")\n",
    "    return None\n",
    "\n",
    "def track_resource_usage(application_id):\n",
    "    # Endpoint to get the status of a YARN application\n",
    "    status_url = f\"{yarn_url}/ws/v1/cluster/apps/{application_id}\"\n",
    "\n",
    "    # Track the resource usage of the Hadoop job until it is completed\n",
    "    while True:\n",
    "        response = requests.get(status_url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            app_status = data['app']['finalStatus']\n",
    "            if app_status in ['SUCCEEDED', 'FAILED', 'KILLED']:\n",
    "                break\n",
    "            resource = data['app']['allocatedResources']\n",
    "            print(f\"Allocated vCores: {resource['vCores']}, Allocated Memory: {resource['memory']}\")\n",
    "        time.sleep(5)\n",
    "\n",
    "    if app_status == 'SUCCEEDED':\n",
    "        print(\"Hadoop job completed successfully.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Hadoop job failed or was killed.\")\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "jar_path = '/path/to/hadoop/job.jar'\n",
    "input_path = '/input/path'\n",
    "output_path = '/output/path'\n",
    "vcores = 2\n",
    "memory = 2048\n",
    "\n",
    "application_id = submit_hadoop_job(jar_path, input_path, output_path, vcores, memory)\n",
    "if application_id:\n",
    "    if track_resource_usage(application_id):\n",
    "        print(\"Resource tracking completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time.\n",
    "#Sol:\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "\n",
    "class WordCountJob(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(WordCountJob, self).configure_args()\n",
    "        self.add_passthru_arg('--split_size', type=int, default=128, help='Input split size in MB')\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.strip().split():\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Example usage with different split sizes\n",
    "    split_sizes = [128, 256, 512]\n",
    "\n",
    "    for split_size in split_sizes:\n",
    "        job = WordCountJob(args=['large_text_file.txt', f'--split_size={split_size}'])\n",
    "        with job.make_runner() as runner:\n",
    "            runner.run()\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Split Size: {split_size} MB, Elapsed Time: {elapsed_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
